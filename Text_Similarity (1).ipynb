{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1-cGCN4fzTg"
      },
      "source": [
        "# Doc2Vec Approach\n",
        "\n",
        "I am using google colab (as it is free and fast) to train the gensim doc2Vec model (It can also be trained locally but it might take more time and it can lead to some primary memory space constraints))\n",
        "\n",
        "**Importing the dataset from google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cisktox--47g"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "# drive.mount('/content/drive/')\n",
        "# os.chdir('drive/My Drive/Datasets')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbkGZPP9gIkg"
      },
      "source": [
        "# **Assigning DatasetName**\n",
        "\n",
        "The same dataset name can be kept in local folder. The above code can be ignored, if you want to run this locally, just keep the file in the folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikwAjFmKAvIM"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"/content/Text_Similarity_Dataset.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIK8uO0XgQa6"
      },
      "source": [
        "# **Importing** Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8-rYBCgA1rF",
        "outputId": "3fbde31c-3c45-4cf4-daf8-2a9d827e3e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import gensim\n",
        "import re\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ecWmE0hwiC"
      },
      "source": [
        "# **Loading data to dataframe**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxGUuhQfA6K7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcrnMhs1h3zL"
      },
      "source": [
        "# **Function to preprocess**\n",
        "\n",
        "This function removes stopwords, punctuations and stems the words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyFEVQ2tA86e"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text,remove_stopwords=True):\n",
        "  words = text.lower().split()\n",
        "  # remove stopwords based on flag\n",
        "  if remove_stopwords:\n",
        "    stops = set(stopwords.words('english'))\n",
        "    words = [w for w in words if w not in stops]\n",
        "  sentence = \" \".join(words)\n",
        "  sentence = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`]\", \" \", sentence)\n",
        "  sentence = re.sub(r\"\\'s\", \" 's \", sentence)\n",
        "  sentence = re.sub(r\"\\'ve\", \" 've \", sentence)\n",
        "  sentence = re.sub(r\"n\\'t\", \" 't \", sentence)\n",
        "  sentence = re.sub(r\"\\'re\", \" 're \", sentence)\n",
        "  sentence = re.sub(r\"\\'d\", \" 'd \", sentence)\n",
        "  sentence = re.sub(r\"\\'ll\", \" 'll \", sentence)\n",
        "  sentence = re.sub(r\",\", \" \", sentence)\n",
        "  sentence = re.sub(r\"\\.\", \" \", sentence)\n",
        "  sentence = re.sub(r\"!\", \" \", sentence)\n",
        "  sentence = re.sub(r\"\\(\", \" ( \", sentence)\n",
        "  sentence = re.sub(r\"\\)\", \" ) \", sentence)\n",
        "  sentence = re.sub(r\"\\?\", \" \", sentence)\n",
        "  sentence = re.sub(r\"\\s{2,}\", \" \", sentence)\n",
        "\n",
        "  words = sentence.split()\n",
        "  # Shorten words to their stems\n",
        "  stemmer = SnowballStemmer('english')\n",
        "  stemmed_words = [stemmer.stem(word) for word in words]\n",
        "  return \" \".join(stemmed_words)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhqMc96BimOE"
      },
      "source": [
        "**Preprocess all sentences and make a document array**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwIyPDI0Ep9J"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for index,data in enumerate(df.values):\n",
        "  documents.append(preprocess_text(data[1],True))\n",
        "  documents.append(preprocess_text(data[2],True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id7mpg1PjDUk"
      },
      "source": [
        "**Import Gensim module for document to vector and tagged documents to create dataformat for training the doc2vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l7uH4UiFUAH"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2Vta59LGPSA"
      },
      "outputs": [],
      "source": [
        "# It creates word2Vec for all the words and uses tags as another input to neural network to generate a document vector\n",
        "tagged_documents = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(documents)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq9Y6uj0jmTN"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "\n",
        "\n",
        "1.   You can define the epochs\n",
        "2.   the vector size is the size of the vector space required to represent a document\n",
        "3. alpha is the learning rate for the network\n",
        "4. Doc2Vec class is used to create doc2vec model, this model gives the vector representation for the sentence\n",
        "5. The model is saved so that it can be reused whenever required\n",
        "\n",
        "The learning rate is decayed to avoid large updates\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwI0scyRGY-8",
        "outputId": "95555b19-e84a-4b1b-bd7d-9e9e2d9a0303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1\n",
            "iteration 2\n",
            "iteration 3\n",
            "iteration 4\n",
            "iteration 5\n",
            "iteration 6\n",
            "iteration 7\n",
            "iteration 8\n",
            "iteration 9\n",
            "Model Saved\n"
          ]
        }
      ],
      "source": [
        "def train_and_save(force_train=False,saved_model_name=\"d2v.model\"):\n",
        "  if os.path.exists(saved_model_name) and not force_train:\n",
        "    return\n",
        "  max_epochs = 10\n",
        "  vec_size = 100\n",
        "  alpha = 0.025\n",
        "\n",
        "  model = Doc2Vec(vector_size=vec_size,\n",
        "                  alpha=alpha, \n",
        "                  min_alpha=0.00025,\n",
        "                  min_count=1,\n",
        "                  dm =1)\n",
        "    \n",
        "  model.build_vocab(tagged_documents)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "      print('iteration {0}'.format(epoch))\n",
        "      model.train(tagged_documents,\n",
        "                  total_examples=model.corpus_count,\n",
        "                  epochs=model.iter)\n",
        "      # decrease the learning rate\n",
        "      model.alpha -= 0.0002\n",
        "      # fix the learning rate, no decay\n",
        "      model.min_alpha = model.alpha\n",
        "\n",
        "  model.save(\"saved_model_name\")\n",
        "  print(\"Model Saved\")\n",
        "\n",
        "train_and_save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ixhi3tGkR3I"
      },
      "source": [
        "# Load the model to demonstrate reusability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krMyaZq2Gp9S"
      },
      "outputs": [],
      "source": [
        "model = Doc2Vec.load(\"saved_model_name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2IojhgRkjWh"
      },
      "source": [
        "# Scoring\n",
        "\n",
        "\n",
        "\n",
        "1.   Generate scores for each pair of sentences in dataset (based on some threshold value assign it label 0 (highly similar) or 1 (not similar)\n",
        "2.   put the score back to dataset\n",
        "3. **I have used 70% as threshold for similarity score**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DJeMfZzG5S7",
        "outputId": "37f25f44-fb29-4fd0-e2dc-db50a91bb010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "threshold = 0.70\n",
        "for i in range(0,len(documents),2):\n",
        "  # print(i)\n",
        "  score = model.n_similarity(word_tokenize(documents[i]),word_tokenize(documents[i+1]))\n",
        "  scores.append(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjQxtH8I4QbC"
      },
      "source": [
        "# Save the result to a new file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uVVVc6dd1SI"
      },
      "outputs": [],
      "source": [
        "data = {'Unique_ID':df['Unique_ID'],'Similarity_Score':scores}\n",
        "df_to_save = pd.DataFrame(data=data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5rQh1E22w43"
      },
      "outputs": [],
      "source": [
        "df_to_save.to_csv(\"final_text_similarity_scores.csv\",index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "czINXzdOEEdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Text_Similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}